# Importa y ejecuta la configuraci√≥n de Streamlit antes que cualquier otra cosa
from config_streamlit import set_streamlit_page_config
set_streamlit_page_config()

import streamlit as st
import yaml
import os
import json
import re
from agents import AgentManager
from utilities import (
    initialize_system,
    cache_response,
    get_cached_response,
    summarize_text,
    perform_web_search,
    log_error,
    log_warning,
    log_info,
    evaluate_query_complexity,
)
import time
import asyncio
import polars as pl
import random
import logging
import traceback
from typing import Tuple, Dict, Any, Optional
from load_secrets import load_secrets, get_secret, secrets

# Configuraci√≥n de logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('mallo.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Carga todos los secretos al inicio de la aplicaci√≥n
load_secrets()

# CSS personalizado para interfaz moderna - A√±adir despu√©s de set_streamlit_page_config()
st.markdown("""
<style>
    /* Estilos base y temas */
    .main {
        background-color: transparent !important;
    }
    .stApp {
        background-color: transparent !important;
    }
    .stMarkdown h1 {
        color: inherit;
    }
    
    /* Componentes de chat */
    .stChatMessage {
        background-color: rgba(255, 255, 255, 0.05);
        border-radius: 15px;
        padding: 1rem;
        margin: 0.5rem 0;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }
    
    /* Botones y controles */
    .export-button {
        background-color: rgba(240, 242, 246, 0.1);
        border: none;
        border-radius: 5px;
        padding: 0.5rem 1rem;
        margin: 0.25rem;
        cursor: pointer;
    }
    
    /* Paneles y evaluaciones */
    .ethical-evaluation {
        background-color: rgba(248, 249, 250, 0.05);
        border-left: 4px solid #00cc66;
        padding: 1rem;
    }
    .reasoning-chain {
        background-color: rgba(248, 249, 250, 0.05);
        border-left: 4px solid #0066cc;
        padding: 1rem;
    }
    
    /* Expansores */
    div[data-testid="stExpander"] {
        border: 1px solid rgba(240, 242, 246, 0.1);
        border-radius: 10px;
        margin-top: 0.5rem;
    }

    /* M√©tricas y badges */
    .metric-card {
        background-color: rgba(255, 255, 255, 0.05);
        padding: 1rem;
        border-radius: 8px;
        margin: 0.5rem 0;
    }
    
    /* Enlaces y navegaci√≥n */
    .social-links a {
        color: inherit;
        text-decoration: none;
        padding: 0.25rem 0.5rem;
        border-radius: 4px;
        transition: background-color 0.3s;
    }
    
    .social-links a:hover {
        background-color: rgba(255, 255, 255, 0.1);
    }
</style>
""", unsafe_allow_html=True)

def load_config():
    """Carga la configuraci√≥n desde el archivo YAML."""
    try:
        with open('config.yaml', 'r', encoding='utf-8') as file:
            return yaml.safe_load(file)
    except FileNotFoundError:
        st.error("No se pudo encontrar el archivo de configuraci√≥n 'config.yaml'.")
        st.stop()
    except yaml.YAMLError as e:
        st.error(f"Error al leer el archivo de configuraci√≥n: {str(e)}")
        st.stop()

def load_speed_test_results():
    """Carga los resultados de las pruebas de velocidad."""
    try:
        with open('model_speeds.json', 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        return None

def display_speed_test_results(results):
    """Muestra los resultados de las pruebas de velocidad en la barra lateral."""
    data = [
        {"API": api, "Modelo": model['model'], "Velocidad": f"{model['speed']:.4f}"}
        for api, models in results.items()
        for model in models
    ]
    df = pl.DataFrame(data).sort("Velocidad")
    with st.sidebar.expander("üìä Resultados de Velocidad", expanded=False):
        st.dataframe(df, use_container_width=True, hide_index=True)

def initialize_session_state():
    """Inicializa el estado de la sesi√≥n con valores predeterminados."""
    default_states = {
        "messages": [],
        "context": "",
        "show_settings": False,
        "last_details": {},
        "error_count": 0,
        "last_successful_response": None
    }
    
    for key, value in default_states.items():
        if key not in st.session_state:
            st.session_state[key] = value

def export_conversation_to_md(messages, details):
    """Exporta la conversaci√≥n completa a Markdown."""
    md_content = "# Conversaci√≥n con MALLO\n\n"
    md_content += f"Fecha: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n"
    
    for message in messages:
        if message["role"] == "user":
            md_content += f"## üë§ Usuario\n\n{message['content']}\n\n"
        elif message["role"] == "assistant":
            md_content += f"## ü§ñ Asistente\n\n{message['content']}\n\n"
            
            if details:
                md_content += "### üîç Detalles del Proceso\n\n"
                md_content += f"#### Razonamiento\n\n{details['initial_evaluation']}\n\n"
                md_content += f"#### Evaluaci√≥n √âtica\n\n```json\n{json.dumps(details['ethical_evaluation'], indent=2)}\n```\n\n"
                
                if details.get("improved_response"):
                    md_content += f"#### ‚ú® Respuesta Mejorada\n\n{details['improved_response']}\n\n"
                
                if details.get("meta_analysis"):
                    md_content += f"#### üîÑ Meta-an√°lisis\n\n{details['meta_analysis']}\n\n"
                
                md_content += f"#### üìù M√©tricas de Rendimiento\n\n```json\n{json.dumps(details['performance_metrics'], indent=2)}\n```\n\n"
                md_content += f"#### üåê Contexto de la Conversaci√≥n\n\n{st.session_state.get('context', 'No disponible')}\n\n"
                md_content += "---\n\n"

    timestamp = time.strftime("%Y%m%d-%H%M%S")
    filename = f"mallo_conversation_{timestamp}.md"
    temp_dir = "temp"
    os.makedirs(temp_dir, exist_ok=True)
    filepath = os.path.join(temp_dir, filename)
    
    with open(filepath, "w", encoding="utf-8") as f:
        f.write(md_content)
    
    return filepath, filename

def render_response_details(details: Dict):
    """Renderiza los detalles de la respuesta en expansores organizados."""
    col1, col2 = st.columns(2)
    
    with col1:
        with st.expander("üí≠ Proceso de Razonamiento", expanded=False):
            st.markdown(details["initial_evaluation"])
            st.download_button(
                "üì• Exportar Razonamiento",
                details["initial_evaluation"],
                "razonamiento.md",
                mime="text/markdown"
            )
    
    with col2:
        with st.expander("‚öñÔ∏è Evaluaci√≥n √âtica", expanded=False):
            st.json(details["ethical_evaluation"])
            if details.get("improved_response"):
                st.info("Respuesta mejorada √©ticamente:")
                st.write(details["improved_response"])

def render_sidebar_content(system_status: Dict[str, bool], speed_test_results: Optional[Dict]):
    """Renderiza el contenido de la barra lateral con dise√±o mejorado."""
    with st.sidebar:
        # Cabecera
        col1, col2 = st.columns([1, 3])
        with col1:
            st.image("assets/logo.jpg", width=50)
        with col2:
            st.markdown("### MALLO")
            st.caption("MultiAgent LLM Orchestrator")
        
        # M√©tricas principales
        st.markdown("---")
        col1, col2 = st.columns(2)
        with col1:
            st.metric("Agentes", "10+")
        with col2:
            st.metric("Modelos", "20+")
        
        # Enlaces principales
        st.markdown("""
        [![ver c√≥digo fuente](https://img.shields.io/badge/Repositorio%20GitHub-gris?logo=github)](https://github.com/bladealex9848/MALLO)
        ![Visitantes](https://api.visitorbadge.io/api/visitors?path=https%3A%2F%2Fmallollm.streamlit.app&label=Visitantes&labelColor=%235d5d5d&countColor=%231e7ebf&style=flat)
        """)
        
        # Estado del Sistema
        with st.expander("üîß Sistema", expanded=False):
            st.caption("Estado de los componentes")
            for key, value in system_status.items():
                if value:
                    st.success(key, icon="‚úÖ")
                else:
                    st.error(key, icon="‚ùå")
            
            if speed_test_results:
                st.markdown("##### Rendimiento")
                display_speed_test_results(speed_test_results)
        
        # Capacidades
        with st.expander("üí° Capacidades", expanded=False):
            features = {
                "ü§ñ M√∫ltiples Modelos": "Integraci√≥n con principales proveedores de IA",
                "üîç An√°lisis Contextual": "Comprensi√≥n profunda de consultas",
                "üåê B√∫squeda Web": "Informaci√≥n actualizada en tiempo real",
                "‚öñÔ∏è Evaluaci√≥n √âtica": "Respuestas alineadas con principios √©ticos",
                "üîÑ Meta-an√°lisis": "S√≠ntesis de m√∫ltiples fuentes",
                "üéØ Prompts Adaptados": "Especializaci√≥n por tipo de consulta"
            }
            
            for title, description in features.items():
                st.markdown(f"**{title}**")
                st.caption(description)
        
        # Acerca de
        with st.expander("‚ÑπÔ∏è Acerca de", expanded=False):
            st.markdown("""
            MALLO es un orquestador avanzado de IAs que selecciona y coordina 
            m√∫ltiples modelos de lenguaje para proporcionar respuestas √≥ptimas 
            basadas en el contexto y complejidad de cada consulta.
            """)
        
        # Desarrollador
        st.markdown("---")
        st.markdown("### üë®‚Äçüíª Desarrollador")
        
        col1, col2 = st.columns([1, 2])
        with col1:
            st.image("assets/profile.jpg", width=60)
        with col2:
            st.markdown("#### Alexander Oviedo Fadul")
            st.caption("Developer & Legal Tech")
        
        # Enlaces sociales
        st.markdown("##### Contacto")
        social_links = {
            "üåê Website": "https://alexanderoviedofadul.dev",
            "üíº LinkedIn": "https://linkedin.com/in/alexander-oviedo-fadul",
            "üì± WhatsApp": "https://wa.me/573015930519",
            "üìß Email": "mailto:alexander.oviedo.fadul@gmail.com",
            "üêô GitHub": "https://github.com/bladealex9848"
        }
        
        for platform, link in social_links.items():
            st.markdown(f"[{platform}]({link})")

# Resumen de la conversaci√≥n con un l√≠mite de longitud m√°ximo (500 caracteres)
# Si el contexto supera el l√≠mite, se utiliza cohere o OpenAI para resumirlo
def summarize_conversation(
    previous_context, user_input, response, agent_manager, config, max_length=500
):
    new_content = f"Usuario: {user_input}\nAsistente: {response}"
    updated_context = f"{previous_context}\n\n{new_content}".strip()

    if len(updated_context) > max_length:
        summary_prompt = (
            f"Resume la siguiente conversaci√≥n manteniendo los puntos clave:\n\n{updated_context}"
        )

        try:
            # Intenta usar openrouter primero
            # summary = agent_manager.process_with_openrouter(summary_prompt, config['openrouter']['fast_models'])
            # Intenta usar cohere primero
            summary = agent_manager.process_query(
                summary_prompt, "cohere", config["cohere"]["default_model"]
            )
        except Exception as e:
            log_error(
                f"Error al usar cohere para resumen: {str(e)}. Usando OpenAI como respaldo."
            )
            try:
                # Si cohere falla, usa OpenAI como respaldo
                summary = agent_manager.process_query(
                    summary_prompt, "api", config["openai"]["default_model"]
                )
            except Exception as e:
                log_error(
                    f"Error al usar OpenAI para resumen: {str(e)}. Devolviendo contexto sin resumir."
                )
                return updated_context

        return summary

    return updated_context

def evaluate_ethical_compliance(response: str, prompt_type: str) -> Dict[str, Any]:
    """
    Eval√∫a el cumplimiento √©tico y legal de la respuesta generada.

    Args:
    response (str): La respuesta generada por el sistema.
    prompt_type (str): El tipo de prompt utilizado para generar la respuesta.

    Returns:
    Dict[str, Any]: Un diccionario con los resultados de la evaluaci√≥n.
    """
    evaluation = {
        "sesgo_detectado": False,
        "privacidad_respetada": True,
        "transparencia": True,
        "alineacion_derechos_humanos": True,
        "responsabilidad": True,
        "explicabilidad": True,
    }

    # Verificar sesgos (esto requerir√≠a un modelo m√°s sofisticado en la pr√°ctica)
    if any(
        palabra in response.lower()
        for palabra in ["todos los hombres", "todas las mujeres"]
    ):
        evaluation["sesgo_detectado"] = True

    # Verificar privacidad (ejemplo simplificado)
    if any(
        dato in response
        for dato in ["n√∫mero de identificaci√≥n", "direcci√≥n", "tel√©fono"]
    ):
        evaluation["privacidad_respetada"] = False

    # Verificar transparencia
    if "Esta respuesta fue generada por IA" not in response:
        evaluation["transparencia"] = False

    # La alineaci√≥n con derechos humanos, responsabilidad y explicabilidad
    # requerir√≠an an√°lisis m√°s complejos en un sistema real

    return evaluation

# Evaluar la respuesta del modelo de lenguaje y proporcionar retroalimentaci√≥n
def evaluate_response(agent_manager, config, evaluation_type, query, response=None):
    eval_config = config["evaluation_models"][evaluation_type]

    if evaluation_type == "initial":
        evaluation_prompt = f"""
        Analiza la siguiente consulta y proporciona una gu√≠a detallada para responderla:

        Consulta: {query}

        Tu tarea es:
        1. Identificar los puntos clave que deben abordarse en la respuesta.
        2. Determinar si se necesita informaci√≥n actualizada o reciente para responder adecuadamente. Si es as√≠, indica "BUSQUEDA_WEB: SI" en tu respuesta.
        3. Evaluar la complejidad de la consulta en una escala de 0 a 1, donde 0 es muy simple y 1 es muy compleja. Indica "COMPLEJIDAD: X" donde X es el valor num√©rico.
        4. Decidir si la consulta requiere conocimientos de m√∫ltiples dominios o fuentes. Si es as√≠, indica "MOA: SI" en tu respuesta.
        5. Sugerir fuentes de informaci√≥n relevantes para la consulta.
        6. Proponer un esquema o estructura para la respuesta.
        7. Indicar cualquier consideraci√≥n especial o contexto importante para la consulta.

        Por favor, proporciona tu an√°lisis y gu√≠a en un formato claro y estructurado.
        """
    else:  # evaluation_type == 'final'
        evaluation_prompt = f"""
        Eval√∫a la siguiente respuesta a la consulta dada:

        Consulta: {query}

        Respuesta:
        {response}

        Tu tarea es:
        1. Determinar si la respuesta es apropiada y precisa para la consulta.
        2. Identificar cualquier informaci√≥n faltante o imprecisa.
        3. Evaluar la claridad y estructura de la respuesta.
        4. Si es necesario, proporcionar una versi√≥n mejorada de la respuesta.

        Por favor, proporciona tu evaluaci√≥n en un formato claro y estructurado, incluyendo una versi√≥n mejorada de la respuesta si lo consideras necesario.
        """

    for attempt in range(3):  # Intentar hasta 3 veces
        try:
            if attempt == 0:
                evaluation = agent_manager.process_query(
                    evaluation_prompt, eval_config["api"], eval_config["model"]
                )
            elif attempt == 1:
                evaluation = agent_manager.process_query(
                    evaluation_prompt,
                    eval_config["backup_api"],
                    eval_config["backup_model"],
                )
            else:
                evaluation = agent_manager.process_query(
                    evaluation_prompt,
                    eval_config["backup_api2"],
                    eval_config["backup_model2"],
                )

            if "Error al procesar" not in evaluation:
                return evaluation
        except Exception as e:
            log_error(
                f"Error en evaluaci√≥n {evaluation_type} con {'modelo principal' if attempt == 0 else 'modelo de respaldo'}: {str(e)}"
            )

    return "No se pudo realizar la evaluaci√≥n debido a m√∫ltiples errores en los modelos de evaluaci√≥n."

def process_user_input(user_input: str, config: Dict[str, Any], agent_manager: Any) -> Tuple[str, Dict[str, Any]]:
    """
    Procesa la entrada del usuario y genera una respuesta usando los agentes apropiados.
    
    Args:
        user_input (str): La consulta del usuario
        config (Dict[str, Any]): Configuraci√≥n del sistema
        agent_manager (Any): Instancia del gestor de agentes
    
    Returns:
        Tuple[str, Dict[str, Any]]: (respuesta, detalles del procesamiento)
    """
    try:
        conversation_context = st.session_state.get('context', '')
        enriched_query = f"{conversation_context}\n\nNueva consulta: {user_input}"

        # Verificar cach√©
        cached_response = get_cached_response(enriched_query)
        if cached_response:
            st.success("üíæ Respuesta recuperada de cach√©")
            return cached_response

        progress_placeholder = st.empty()
        
        # Iniciar procesamiento
        start_time = time.time()
        progress_placeholder.write("üîç Evaluando consulta...")
        
        # Evaluaci√≥n inicial
        initial_evaluation = evaluate_response(
            agent_manager, config, 'initial', enriched_query
        )
        
        # An√°lisis de complejidad y necesidades
        complexity, needs_web_search, needs_moa, prompt_type = evaluate_query_complexity(
            initial_evaluation, ""
        )
        prompt_type = agent_manager.validate_prompt_type(user_input, prompt_type)
        
        # B√∫squeda web si es necesaria
        web_context = ""
        if needs_web_search:
            progress_placeholder.write("üåê Realizando b√∫squeda web...")
            web_context = perform_web_search(user_input)
            enriched_query = f"{enriched_query}\nContexto web: {web_context}"
        
        # Selecci√≥n de agentes
        progress_placeholder.write("ü§ñ Seleccionando agentes...")
        specialized_agent = agent_manager.select_specialized_agent(enriched_query)
        general_agents = agent_manager.get_prioritized_agents(
            enriched_query, complexity, prompt_type
        )

        # Priorizaci√≥n de agentes
        prioritized_agents = []
        if specialized_agent:
            prioritized_agents.append((
                specialized_agent['type'],
                specialized_agent['id'],
                specialized_agent['name']
            ))

        for agent in general_agents:
            if len(prioritized_agents) >= 2:
                break
            if agent not in prioritized_agents:
                prioritized_agents.append(agent)

        prioritized_agents = prioritized_agents[:2]
        
        # Procesamiento con agentes
        agent_results = []
        for agent_type, agent_id, agent_name in prioritized_agents:
            progress_placeholder.write(f"‚öôÔ∏è Procesando con {agent_name}...")
            try:
                enriched_query_with_prompt = agent_manager.apply_specialized_prompt(
                    enriched_query, prompt_type
                )
                result = agent_manager.process_query(
                    enriched_query_with_prompt,
                    agent_type,
                    agent_id,
                    prompt_type
                )
                agent_results.append({
                    "agent": agent_type,
                    "model": agent_id,
                    "name": agent_name,
                    "status": "success",
                    "response": result
                })
            except Exception as e:
                logger.error(f"Error en el procesamiento con {agent_name}: {str(e)}")
                agent_results.append({
                    "agent": agent_type,
                    "model": agent_id,
                    "name": agent_name,
                    "status": "error",
                    "response": str(e)
                })

        # Verificar respuestas exitosas
        successful_responses = [
            r for r in agent_results if r["status"] == "success"
        ]
        
        if not successful_responses:
            raise ValueError("No se pudo obtener una respuesta v√°lida de ning√∫n agente")

        # Meta-an√°lisis si es necesario
        if needs_moa and len(successful_responses) > 1:
            progress_placeholder.write("üîÑ Realizando meta-an√°lisis...")
            meta_analysis_result = agent_manager.meta_analysis(
                user_input,
                [r["response"] for r in successful_responses],
                initial_evaluation,
                ""
            )
            final_response = agent_manager.process_query(
                f"Bas√°ndote en este meta-an√°lisis, proporciona una respuesta conversacional y directa a la pregunta '{user_input}'. La respuesta debe ser natural, como si estuvieras charlando con un amigo, sin usar frases como 'Bas√°ndome en el an√°lisis' o 'La respuesta es'. Simplemente responde de manera clara y concisa: {meta_analysis_result}",
                agent_manager.meta_analysis_api,
                agent_manager.meta_analysis_model
            )
        else:
            final_response = successful_responses[0]["response"]

        # Evaluaci√≥n √©tica
        progress_placeholder.write("‚öñÔ∏è Evaluando cumplimiento √©tico...")
        ethical_evaluation = evaluate_ethical_compliance(final_response, prompt_type)
        
        # Mejora √©tica si es necesaria
        if any(not value for value in ethical_evaluation.values()):
            progress_placeholder.write("‚ú® Mejorando respuesta...")
            specialized_assistant = agent_manager.get_specialized_assistant(
                'asst_F33bnQzBVqQLcjveUTC14GaM'
            )
            enhancement_prompt = f"""
            Analiza la siguiente respuesta y su evaluaci√≥n √©tica:

            Respuesta: {final_response}

            Evaluaci√≥n √©tica: {json.dumps(ethical_evaluation, indent=2)}

            Por favor, modifica la respuesta para mejorar su alineaci√≥n con principios √©ticos y legales,
            abordando cualquier preocupaci√≥n identificada en la evaluaci√≥n. Aseg√∫rate de que la respuesta sea
            transparente sobre el uso de IA, libre de sesgos, y respetuosa de los derechos humanos y la privacidad.
            """
            improved_response = agent_manager.process_query(
                enhancement_prompt,
                'assistant',
                specialized_assistant['id']
            )
            improved_ethical_evaluation = evaluate_ethical_compliance(
                improved_response,
                prompt_type
            )
        else:
            improved_response = None
            improved_ethical_evaluation = None

        # Evaluaci√≥n final
        progress_placeholder.write("üìù Evaluaci√≥n final...")
        final_evaluation = evaluate_response(
            agent_manager,
            config,
            'final',
            user_input,
            final_response
        )

        # C√°lculo del tiempo de procesamiento
        processing_time = time.time() - start_time

        # Preparar detalles de la respuesta
        details = {
            "selected_agents": [
                {
                    "agent": r["agent"],
                    "model": r["model"],
                    "name": r["name"]
                } for r in agent_results
            ],
            "processing_time": f"{processing_time:.2f} segundos",
            "complexity": complexity,
            "needs_web_search": needs_web_search,
            "needs_moa": needs_moa,
            "web_context": web_context,
            "prompt_type": prompt_type,
            "initial_evaluation": initial_evaluation,
            "agent_processing": agent_results,
            "final_evaluation": final_evaluation,
            "ethical_evaluation": ethical_evaluation,
            "improved_response": improved_response,
            "improved_ethical_evaluation": improved_ethical_evaluation,
            "performance_metrics": {
                "total_agents_called": len(agent_results),
                "successful_responses": len(successful_responses),
                "failed_responses": len(agent_results) - len(successful_responses),
                "average_response_time": f"{processing_time:.2f} segundos"
            },
            "meta_analysis": meta_analysis_result if needs_moa and len(successful_responses) > 1 else None,
            "final_response": final_response
        }

        # Actualizar contexto
        new_context = summarize_conversation(
            conversation_context,
            user_input,
            final_response,
            agent_manager,
            config
        )
        st.session_state["context"] = new_context

        # Guardar en cach√©
        cache_response(enriched_query, (final_response, details))

        # Limpiar placeholder de progreso
        progress_placeholder.empty()
        
        return final_response, details

    except Exception as e:
        logger.error(f"Error en process_user_input: {str(e)}")
        logger.error(traceback.format_exc())
        return (
            "Lo siento, ha ocurrido un error al procesar tu consulta. Por favor, intenta de nuevo.",
            {"error": str(e)}
        )

def main():
    try:
        # Inicializaci√≥n
        initialize_session_state()
        config = load_config()
        system_status = initialize_system(config)
        agent_manager = AgentManager(config)
        speed_test_results = load_speed_test_results()

        # Renderizar barra lateral
        render_sidebar_content(system_status, speed_test_results)

        # Interfaz principal
        st.title("MALLO: MultiAgent LLM Orchestrator")

        # Interfaz de chat
        for message in st.session_state["messages"]:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])
                if message["role"] == "assistant":
                    with st.expander("üí° Opciones", expanded=False):
                        col1, col2 = st.columns(2)
                        with col1:
                            if st.download_button(
                                "üì• Exportar Conversaci√≥n Completa",
                                *export_conversation_to_md(
                                    st.session_state["messages"],
                                    st.session_state.get("last_details", {})
                                ),
                                mime="text/markdown"
                            ):
                                st.success("Conversaci√≥n exportada exitosamente")
                        with col2:
                            if st.download_button(
                                "üì• Exportar √öltima Respuesta",
                                message["content"],
                                f"respuesta_{time.strftime('%Y%m%d-%H%M%S')}.md",
                                mime="text/markdown"
                            ):
                                st.success("Respuesta exportada exitosamente")
                        render_response_details(st.session_state["last_details"])

        # Input del usuario y manejo de respuestas
        user_input = st.chat_input("¬øEn qu√© puedo ayudarte hoy?")

        if user_input:
            try:
                # Mostrar indicador de procesamiento
                with st.spinner("Procesando tu consulta..."):
                    response, details = process_user_input(user_input, config, agent_manager)
                    st.session_state["last_details"] = details
                    st.session_state["error_count"] = 0  # Resetear contador de errores

                    if response:
                        # Actualizar el historial de la conversaci√≥n
                        st.session_state["messages"].append({
                            "role": "user",
                            "content": user_input,
                            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
                        })
                        st.session_state["messages"].append({
                            "role": "assistant",
                            "content": response,
                            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
                        })
                        st.session_state["last_successful_response"] = response

                        # Mostrar la respuesta con sus detalles
                        with st.chat_message("assistant"):
                            st.markdown(response)
                            
                            # Expandir para mostrar detalles y opciones
                            with st.expander("üîç Detalles y Opciones", expanded=False):
                                # M√©tricas de rendimiento
                                col1, col2, col3 = st.columns(3)
                                with col1:
                                    st.metric("Tiempo de Proceso", 
                                            f"{float(details['processing_time'].split()[0]):.2f}s")
                                with col2:
                                    st.metric("Agentes Usados", 
                                            str(details['performance_metrics']['total_agents_called']))
                                with col3:
                                    st.metric("Complejidad", 
                                            f"{details['complexity']:.2f}")

                                # Botones de exportaci√≥n
                                col1, col2 = st.columns(2)
                                with col1:
                                    if st.download_button(
                                        "üì• Exportar Conversaci√≥n Completa",
                                        *export_conversation_to_md(
                                            st.session_state["messages"],
                                            st.session_state.get("last_details", {})
                                        ),
                                        mime="text/markdown"
                                    ):
                                        st.success("‚úÖ Conversaci√≥n exportada exitosamente")
                                
                                with col2:
                                    if st.download_button(
                                        "üì• Exportar √öltima Respuesta",
                                        response,
                                        f"respuesta_{time.strftime('%Y%m%d-%H%M%S')}.md",
                                        mime="text/markdown"
                                    ):
                                        st.success("‚úÖ Respuesta exportada exitosamente")

                                # Mostrar detalles adicionales
                                st.markdown("#### üìä Detalles del Procesamiento")
                                render_response_details(details)

                                # Informaci√≥n sobre el contexto
                                st.markdown("#### üîÑ Contexto de la Conversaci√≥n")
                                with st.expander("Ver contexto actual", expanded=False):
                                    st.text(st.session_state.get("context", "No hay contexto disponible"))

            except Exception as e:
                st.session_state["error_count"] += 1
                logger.error(f"Error procesando entrada del usuario: {str(e)}")
                logger.error(traceback.format_exc())
                
                # Mensaje de error adaptativo
                error_message = (
                    "Ha ocurrido un error al procesar tu consulta. "
                    "Por favor, intenta de nuevo o reformula tu pregunta."
                )
                
                # Mensajes adicionales basados en el contador de errores
                if st.session_state["error_count"] > 3:
                    error_message += (
                        "\n\n‚ö†Ô∏è Parece que estamos teniendo problemas t√©cnicos persistentes. "
                        "Te sugerimos:\n"
                        "1. Intentar m√°s tarde\n"
                        "2. Verificar tu conexi√≥n a internet\n"
                        "3. Contactar al soporte t√©cnico\n"
                        "\nPuedes continuar la conversaci√≥n con el √∫ltimo contexto exitoso."
                    )
                    
                    # Intentar recuperar el √∫ltimo estado exitoso
                    if st.session_state.get("last_successful_response"):
                        st.info(
                            "üëâ Mientras tanto, puedes revisar la √∫ltima respuesta exitosa "
                            "o exportar la conversaci√≥n hasta este punto."
                        )
                
                st.error(error_message)
                
                # Log del error para an√°lisis
                logger.error(f"Error Count: {st.session_state['error_count']}")
                logger.error(f"Last Successful Response Available: {bool(st.session_state.get('last_successful_response'))}")

    except Exception as e:
        logger.error(f"Error cr√≠tico en la aplicaci√≥n: {str(e)}")
        logger.error(traceback.format_exc())
        
        # Mensaje de error amigable pero informativo
        st.error(
            "üö® Error cr√≠tico en la aplicaci√≥n\n\n"
            "Ha ocurrido un error inesperado. Por favor:\n"
            "1. Recarga la p√°gina\n"
            "2. Verifica tu conexi√≥n\n"
            "3. Si el problema persiste, contacta al soporte t√©cnico\n\n"
            "Tus datos de conversaci√≥n est√°n seguros y se intentar√°n recuperar en la pr√≥xima sesi√≥n."
        )
        
        # Intentar guardar el estado actual para recuperaci√≥n
        try:
            with open('error_recovery.json', 'w') as f:
                json.dump({
                    "messages": st.session_state.get("messages", []),
                    "context": st.session_state.get("context", ""),
                    "last_details": st.session_state.get("last_details", {}),
                    "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
                }, f)
        except Exception as save_error:
            logger.error(f"Error al guardar estado para recuperaci√≥n: {str(save_error)}")

if __name__ == "__main__":
    main()