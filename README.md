![Logo de MALLO](https://raw.githubusercontent.com/bladealex9848/MALLO/main/assets/logo.jpg)

# MALLO: MultiAgent LLM Orchestrator

## Tabla de Contenidos
1. [Descripci√≥n](#descripci√≥n)
2. [Caracter√≠sticas Principales](#caracter√≠sticas-principales)
3. [Estructura del Proyecto](#estructura-del-proyecto)
4. [Requisitos Previos](#requisitos-previos)
5. [Instalaci√≥n](#instalaci√≥n)
6. [Configuraci√≥n](#configuraci√≥n)
7. [Uso](#uso)
8. [Evaluaci√≥n √âtica y de Cumplimiento](#evaluaci√≥n-√©tica-y-de-cumplimiento)
9. [TIPS y Trucos](#tips-y-trucos)
10. [Arquitectura del Sistema](#arquitectura-del-sistema)
11. [Componentes Principales](#componentes-principales)
12. [Flujo de Trabajo](#flujo-de-trabajo)
13. [APIs y Servicios Integrados](#apis-y-servicios-integrados)
14. [Manejo de Errores y Logging](#manejo-de-errores-y-logging)
15. [Optimizaci√≥n y Cach√©](#optimizaci√≥n-y-cach√©)
16. [Pruebas](#pruebas)
17. [Contribuci√≥n](#contribuci√≥n)
18. [Registro de Cambios](#registro-de-cambios)
19. [ExperimentaLABs](#experimentalabs)
20. [Roadmap](#roadmap)
21. [Licencia](#licencia)
22. [Contacto](#contacto)

## Descripci√≥n

MALLO (MultiAgent LLM Orchestrator) es una plataforma avanzada de orquestaci√≥n de modelos de lenguaje que implementa una arquitectura distribuida basada en agentes. Dise√±ada espec√≠ficamente para el procesamiento de consultas complejas, MALLO utiliza un sistema de selecci√≥n din√°mica de agentes que combina:

- Modelos de lenguaje locales para procesamiento de baja latencia
- APIs de modelos en la nube para tareas complejas
- Asistentes especializados para dominios espec√≠ficos
- Sistemas de b√∫squeda y enriquecimiento de contexto

La arquitectura del sistema se basa en principios de dise√±o modular y extensible, permitiendo la integraci√≥n seamless de nuevos modelos y servicios. El n√∫cleo del sistema implementa algoritmos avanzados de evaluaci√≥n de complejidad y selecci√≥n de agentes, optimizando la relaci√≥n entre precisi√≥n, latencia y consumo de recursos.

## Caracter√≠sticas Principales

### 1. Arquitectura Multi-Agente Avanzada
- **Procesamiento Distribuido**:
  - Pipeline de procesamiento paralelo para m√∫ltiples agentes
  - Sistema de votaci√≥n y consenso para respuestas m√∫ltiples
  - Mecanismos de fallback y recuperaci√≥n autom√°tica
  - Sistema robusto de manejo de errores para APIs externas
  - Selecci√≥n autom√°tica de modelos basada en el tipo de consulta
  - Personalizaci√≥n de etapas de procesamiento (evaluaci√≥n inicial, b√∫squeda web, meta-an√°lisis, evaluaci√≥n √©tica)

- **Integraci√≥n de Modelos**:
  - Modelos locales v√≠a Ollama para baja latencia (offline-capable)
  - APIs cloud premium (OpenAI, Anthropic, Groq)
  - Proveedores especializados (Together, DeepInfra, DeepSeek)
  - Modelos open-source optimizados (Mistral, Cohere)
  - Acceso a modelos avanzados v√≠a OpenRouter con manejo robusto de errores
  - Soporte para modelos multimodales (Llama 4 Maverick/Scout)
  - Sistema de respaldo para modelos que fallan

### 2. Sistema de Procesamiento Inteligente
- **An√°lisis de Complejidad**:
  - Evaluaci√≥n heur√≠stica de consultas
  - Detecci√≥n autom√°tica de dominio y contexto
  - Selecci√≥n din√°mica de agentes basada en m√©tricas
  - Identificaci√≥n de tipo de prompt para procesamiento especializado
  - Determinaci√≥n autom√°tica de necesidad de b√∫squeda web y meta-an√°lisis

- **Optimizaci√≥n de Recursos**:
  - Sistema de cach√© multinivel con validaci√≥n
  - Balanceo din√°mico de carga entre agentes
  - Gesti√≥n inteligente de cuotas y l√≠mites de API
  - Inicializaci√≥n eficiente de componentes con @st.cache_resource
  - Sistema de respaldo para APIs no disponibles

### 3. Capacidades Avanzadas de B√∫squeda y Contextualizaci√≥n
- **Motor de B√∫squeda Multi-Fuente**:
  - Integraci√≥n primaria con API de YOU
  - Fallback a Tavily para b√∫squedas especializadas
  - Sistema de respaldo con DuckDuckGo
  - Agregaci√≥n y deduplicaci√≥n de resultados
  - Visualizaci√≥n detallada de resultados de b√∫squeda en pesta√±a dedicada
  - Identificaci√≥n autom√°tica del proveedor de b√∫squeda utilizado

- **Procesamiento de Contexto**:
  - An√°lisis sem√°ntico de consultas
  - Extracci√≥n de entidades y relaciones
  - Generaci√≥n de embeddings para b√∫squeda contextual
  - Procesamiento de documentos con OCR mediante Mistral
  - Carga de archivos integrada directamente en el chat

### 4. Framework de Evaluaci√≥n y Mejora
- **Sistema de Evaluaci√≥n √âtica**:
  - Detecci√≥n autom√°tica de sesgos
  - Validaci√≥n de privacidad y seguridad
  - Alineaci√≥n con principios √©ticos configurables
  - Opci√≥n para activar/desactivar la evaluaci√≥n √©tica
  - Visualizaci√≥n detallada de resultados de evaluaci√≥n √©tica

- **Mecanismos de Feedback**:
  - Evaluaci√≥n continua de calidad de respuestas
  - Sistema de aprendizaje basado en retroalimentaci√≥n
  - M√©tricas de rendimiento en tiempo real
  - Exportaci√≥n detallada de todo el proceso de an√°lisis
  - Meta-an√°lisis configurable para s√≠ntesis de m√∫ltiples fuentes

### 5. Interfaz y Monitoreo
- **UI Moderna y Responsive**:
  - Framework Streamlit con dise√±o optimizado
  - Componentes din√°micos de visualizaci√≥n
  - Sistema de progreso en tiempo real
  - Interfaz simplificada con carga de documentos integrada en el chat
  - Personalizaci√≥n de etapas de procesamiento y selecci√≥n de modelos
  - Sistema de pesta√±as para organizar la informaci√≥n (Detalles, B√∫squeda Web, Meta-an√°lisis, etc.)
  - Bot√≥n para limpiar la conversaci√≥n y el contexto sin refrescar la p√°gina
  - Exportaci√≥n mejorada con toda la informaci√≥n del procesamiento

- **Logging y Observabilidad**:
  - Logging estructurado con niveles configurables
  - M√©tricas Prometheus para monitoreo
  - Sistema de alertas para eventos cr√≠ticos
  - Mensajes de error amigables y detallados para el usuario
  - Visualizaci√≥n en tiempo real del proceso de selecci√≥n de modelos

### 6. Configuraci√≥n y Extensibilidad
- **Sistema de Configuraci√≥n Robusto**:
  - Configuraci√≥n centralizada via YAML
  - Soporte para m√∫ltiples entornos
  - Hot-reload de configuraciones
  - Guardado y carga de configuraciones personalizadas
  - Interfaz gr√°fica para personalizar etapas de procesamiento

- **Arquitectura Extensible**:
  - APIs bien documentadas para nuevos agentes
  - Sistema de plugins para funcionalidades adicionales
  - Hooks para personalizaci√≥n de comportamiento
  - Selecci√≥n flexible de modelos principales y de respaldo
  - Organizaci√≥n de modelos por proveedor con nombres descriptivos

### 7. Seguridad y Cumplimiento
- **Protecci√≥n de Datos**:
  - Encriptaci√≥n en tr√°nsito y en reposo
  - Sanitizaci√≥n de entradas y salidas
  - Gesti√≥n segura de credenciales
  - Verificaci√≥n previa de disponibilidad de APIs
  - Sistema robusto de manejo de errores para APIs externas

- **Auditor√≠a y Compliance**:
  - Registro detallado de operaciones
  - Trazabilidad de decisiones del sistema
  - Reportes de cumplimiento normativo
  - Validaci√≥n de respuestas de APIs externas
  - Exportaci√≥n completa de conversaciones con todos los detalles del procesamiento
  - Documentaci√≥n de etapas ejecutadas en cada respuesta

### 8. Optimizaci√≥n de Rendimiento
- **Sistema de Cach√© Avanzado**:
  - Cach√© multinivel (memoria, disco, distribuido)
  - Pol√≠ticas de invalidaci√≥n inteligentes
  - Compresi√≥n y optimizaci√≥n de almacenamiento
  - Inicializaci√≥n eficiente de componentes con @st.cache_resource
  - Carga diferida de recursos no cr√≠ticos

- **Gesti√≥n de Recursos**:
  - L√≠mites de consumo configurables
  - Balanceo de carga autom√°tico
  - Recuperaci√≥n graceful ante fallos
  - Sistema de respaldo para modelos que fallan
  - Selecci√≥n autom√°tica de modelos basada en disponibilidad y tipo de consulta

## Estructura del Proyecto

```
MALLO/
‚îÇ
‚îú‚îÄ‚îÄ agents.py                   # Implementaci√≥n de la clase AgentManager y l√≥gica de agentes
‚îú‚îÄ‚îÄ config.yaml                 # Configuraci√≥n general del sistema
‚îú‚îÄ‚îÄ config_streamlit.py         # Configuraci√≥n de la interfaz de Streamlit
‚îú‚îÄ‚îÄ custom_config.json          # Configuraci√≥n personalizada guardada por el usuario
‚îú‚îÄ‚îÄ document_processor.py       # Procesamiento de documentos y OCR
‚îú‚îÄ‚îÄ error_recovery.json         # Registro de errores para recuperaci√≥n
‚îú‚îÄ‚îÄ load_secrets.py             # Carga de secretos y claves API
‚îú‚îÄ‚îÄ main.py                     # Punto de entrada principal y UI de Streamlit
‚îú‚îÄ‚îÄ model_loader.py             # Carga de modelos desde diferentes fuentes (OpenRouter, Groq, Ollama)
‚îú‚îÄ‚îÄ model_speeds.json           # √çndice de velocidad de modelos locales y en la nube
‚îú‚îÄ‚îÄ README.md                   # Documentaci√≥n del proyecto (este archivo)
‚îú‚îÄ‚îÄ utilities.py                # Funciones de utilidad y helpers
‚îú‚îÄ‚îÄ CHANGELOG.md                # Registro de cambios y versiones
‚îÇ
‚îú‚îÄ‚îÄ .streamlit/                 # Configuraci√≥n de Streamlit
‚îÇ   ‚îî‚îÄ‚îÄ secrets.toml            # Almacenamiento seguro de claves API (no incluido en el repositorio)
‚îÇ
‚îú‚îÄ‚îÄ assets/                     # Directorio para recursos est√°ticos (im√°genes, estilos, etc.)
‚îÇ
‚îú‚îÄ‚îÄ docs/                       # Documentaci√≥n adicional
‚îÇ   ‚îú‚îÄ‚îÄ informes/               # Informes t√©cnicos, de investigaci√≥n y otros documentos
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ ejemplos_consultas/     # Ejemplos de consultas y respuestas para demostraci√≥n
‚îÇ
‚îú‚îÄ‚îÄ legacy_code/                # C√≥digo hist√≥rico y no esencial
‚îÇ   ‚îú‚îÄ‚îÄ cache_manager.py        # Sistema de cach√© (versi√≥n anterior)
‚îÇ   ‚îú‚îÄ‚îÄ cached_init.py          # Inicializaci√≥n en cach√© (versi√≥n anterior)
‚îÇ   ‚îú‚îÄ‚îÄ compatibility_check.py  # Verificaci√≥n de compatibilidad
‚îÇ   ‚îú‚îÄ‚îÄ groq_model_speeds.json  # Velocidades de modelos Groq
‚îÇ   ‚îú‚îÄ‚îÄ mallo.log               # Archivo de registro
‚îÇ   ‚îî‚îÄ‚îÄ mallo_enhancer.py       # L√≥gica experimental para mejorar respuestas
‚îÇ
‚îú‚îÄ‚îÄ static/                     # Archivos est√°ticos para la interfaz
‚îÇ
‚îú‚îÄ‚îÄ temp/                       # Directorio para archivos temporales
‚îÇ
‚îú‚îÄ‚îÄ tools/                      # Utilidades externas
‚îÇ   ‚îú‚îÄ‚îÄ test_groq_model_speed.py    # Script para medir velocidad de modelos Groq
‚îÇ   ‚îú‚îÄ‚îÄ test_model_speeds.py        # Script para medir velocidad de modelos
‚îÇ   ‚îî‚îÄ‚îÄ test_replicate_model_speed.py # Script para medir velocidad de modelos Replicate
‚îÇ
‚îú‚îÄ‚îÄ packages.txt                # Dependencias del sistema
‚îî‚îÄ‚îÄ requirements.txt            # Dependencias de Python
```

## Requisitos Previos

- Python 3.8+
- Ollama instalado localmente (para modelos locales)
- Claves API para servicios externos (OpenAI, Groq, Together, etc.)
- Conexi√≥n a Internet (para b√∫squeda web y APIs externas)

## Instalaci√≥n

1. Clona el repositorio:
   ```
   git clone https://github.com/bladealex9848/MALLO.git
   cd MALLO
   ```

2. Crea y activa un entorno virtual:
   ```
   python3 -m venv venv
   source venv/bin/activate  # En Windows usa `venv\Scripts\activate`
   ```

3. Instala las dependencias:
   ```
   pip install -r requirements.txt
   ```

4. Configura tus claves API:
   Crea un archivo `.streamlit/secrets.toml` y a√±ade tus claves API:
   ```toml
   OPENAI_API_KEY = "tu_clave_openai_aqui"
   GROQ_API_KEY = "tu_clave_groq_aqui"
   TOGETHER_API_KEY = "tu_clave_together_aqui"
   DEEPINFRA_API_KEY="tu_clave_deepinfra_aqui"
   ANTHROPIC_API_KEY="tu_clave_anthropic_aqui"
   DEEPSEEK_API_KEY="tu_clave_deepseek_aqui"
   MISTRAL_API_KEY="tu_clave_mistral_aqui"
   COHERE_API_KEY="tu_clave_cohere_aqui"
   REPLICATE_API_TOKEN= "tu_clave_replicate_aqui"
   OPENROUTER_API_KEY= "tu_clave_openrouter_aqui"
   # A√±ade otras claves API seg√∫n sea necesario
   ```

## Configuraci√≥n

El archivo `config.yaml` contiene la configuraci√≥n principal del sistema. Aqu√≠ puedes ajustar:

- Modelos disponibles para cada proveedor de LLM
- Prioridades de procesamiento
- Umbrales de complejidad para la selecci√≥n de agentes
- Configuraci√≥n de asistentes especializados
- Par√°metros de utilidades como b√∫squeda web y an√°lisis de im√°genes

Ejemplo de configuraci√≥n:

```yaml
ollama:
  base_url: "http://localhost:11434"
  default_model: "gemma2:2b"
  models:
    - "gemma2:2b"
    - "llava:latest"

openai:
  default_model: "gpt-4.1-nano"
  models:
    - "gpt-4.1-nano"

# ... (otras configuraciones)

thresholds:
  moa_complexity: 0.8
  local_complexity: 0.5
  web_search_complexity: 0.4
```

## Uso

Para iniciar la aplicaci√≥n, ejecuta:
```
streamlit run main.py
```

Luego, abre tu navegador y ve a `http://localhost:8501`.

La interfaz de usuario te permitir√°:
1. Ingresar consultas en lenguaje natural y adjuntar archivos directamente en el campo de chat.
2. Ver las respuestas generadas por el sistema.
3. Observar el progreso del procesamiento en tiempo real.
4. Personalizar las etapas de procesamiento (evaluaci√≥n inicial, b√∫squeda web, meta-an√°lisis, evaluaci√≥n √©tica).
5. Seleccionar modelos principales y de respaldo para el procesamiento.
6. Guardar y cargar configuraciones personalizadas.
7. Limpiar la conversaci√≥n y el contexto sin necesidad de refrescar la p√°gina.
8. Exportar la conversaci√≥n completa con todos los detalles del procesamiento.

La interfaz incluye:
- Un chat limpio que muestra las preguntas y respuestas.
- Una barra lateral con opciones de configuraci√≥n y personalizaci√≥n.
- Pesta√±as organizadas para acceder a diferentes tipos de informaci√≥n:
  - **üí° Detalles**: Muestra informaci√≥n sobre el procesamiento interno, incluyendo agentes utilizados y tiempos de respuesta.
  - **üîç B√∫squeda Web**: Presenta los resultados de b√∫squeda web y el proveedor utilizado.
  - **üîÑ Meta-an√°lisis**: Muestra el an√°lisis combinado de m√∫ltiples fuentes cuando est√° activado.
  - **‚öñÔ∏è Evaluaci√≥n √âtica**: Presenta los resultados de la evaluaci√≥n √©tica de cada respuesta.
  - **üí¨ Contexto**: Muestra el contexto acumulado de la conversaci√≥n.
  - **üìö Documentos**: Presenta los documentos procesados cuando se adjuntan archivos.
  - **üìä M√©tricas**: Muestra estad√≠sticas de rendimiento del sistema.
  - **üì§ Exportar**: Permite exportar la conversaci√≥n en diferentes formatos.

## Evaluaci√≥n √âtica y de Cumplimiento en MALLO

MALLO implementa un sistema integral de evaluaci√≥n √©tica y cumplimiento normativo fundamentado en el Acuerdo PCSJA24-12243 del 16 de diciembre de 2024 y los principios del documento CONPES 4144 del 14 de febrero de 2025, que establecen:

### Sistema de Evaluaci√≥n Autom√°tica

La evaluaci√≥n se realiza sobre los siguientes criterios fundamentales:

1.  **Primac√≠a de Derechos Fundamentales**
    * Respeto y protecci√≥n de derechos fundamentales.
    * Promoci√≥n de garant√≠as constitucionales.
    * Alineaci√≥n con mandatos superiores.
2.  **Regulaci√≥n √âtica**
    * Cumplimiento de mandatos constitucionales y legales.
    * Adherencia al C√≥digo Iberoamericano de √âtica.
    * Uso razonable de sistemas de IA.
3.  **Transparencia y Responsabilidad**
    * Identificaci√≥n clara del uso de IA.
    * Explicaci√≥n detallada de fuentes y metodolog√≠as.
    * Documentaci√≥n de procesos y decisiones.
4.  **Seguridad y Privacidad**
    * Protecci√≥n de datos personales.
    * Prevenci√≥n de sesgos y discriminaci√≥n.
    * Mantenimiento de la confidencialidad de la informaci√≥n.
### Proceso de Mejora y Verificaci√≥n

1.  **Control y Verificaci√≥n Humana**
    * Revisi√≥n detallada de resultados.
    * Validaci√≥n de fuentes y referencias.
    * Supervisi√≥n de impactos y consecuencias.
2.  **Proceso de Mejora**
    * Intervenci√≥n del asistente especializado de transformaci√≥n digital (ID: 'asst\_F33bnQzBVqQLcjveUTC14GaM').
    * Refinamiento de respuestas seg√∫n criterios √©ticos.
    * Validaci√≥n de cumplimiento normativo.
### Cumplimiento Normativo

1.  **Marco Legal Aplicable**
    * Sentencia T-323 de 2024 de la Corte Constitucional.
    * Directrices internacionales de gobernanza de IA.
    * Normativas espec√≠ficas del sector judicial.
    * Lineamientos del documento CONPES 4144 de 2025.
2.  **Salvaguardas Espec√≠ficas**
    * Protecci√≥n de datos personales y privacidad.
    * Prevenci√≥n de sesgos y discriminaci√≥n.
    * Garant√≠a de acceso a la justicia.
### Documentaci√≥n y Transparencia

1.  **Registro de Evaluaciones**
    * Documentaci√≥n detallada del proceso evaluativo.
    * Identificaci√≥n de herramientas y modelos utilizados.
    * Registro de modificaciones y mejoras realizadas.
2.  **Acceso a la Informaci√≥n**
    * Disponibilidad de resultados de evaluaci√≥n.
    * Explicaci√≥n de criterios aplicados.
    * Trazabilidad de decisiones y procesos.
Esta implementaci√≥n asegura que MALLO opere bajo los m√°s altos est√°ndares √©ticos y legales, proporcionando un servicio que no solo es t√©cnicamente competente, sino tambi√©n responsable y transparente en su funcionamiento.

## TIPS y Trucos

### Palabras Clave para Prompts Especializados

MALLO est√° dise√±ado para detectar autom√°ticamente el tipo de consulta y aplicar prompts especializados. Sin embargo, puedes ayudar al sistema a seleccionar el prompt m√°s adecuado incluyendo ciertas palabras clave en tu consulta. Aqu√≠ te presentamos algunas sugerencias:

1. **Matem√°ticas**:
   Palabras clave: "c√°lculo", "ecuaci√≥n", "√°lgebra", "geometr√≠a", "estad√≠stica", "probabilidad"

2. **Programaci√≥n**:
   Palabras clave: "c√≥digo", "algoritmo", "funci√≥n", "debug", "software", "desarrollo"

3. **Legal**:
   Palabras clave: "ley", "legislaci√≥n", "jur√≠dico", "contrato", "demanda", "jurisprudencia"

4. **Cient√≠fico**:
   Palabras clave: "experimento", "hip√≥tesis", "teor√≠a", "investigaci√≥n", "m√©todo cient√≠fico"

5. **Hist√≥rico**:
   Palabras clave: "√©poca", "siglo", "per√≠odo", "civilizaci√≥n", "evento hist√≥rico"

6. **Filos√≥fico**:
   Palabras clave: "√©tica", "metaf√≠sica", "epistemolog√≠a", "l√≥gica", "existencialismo"

7. **Contexto Colombiano**:
   Palabras clave: "Colombia", "Bogot√°", "Medell√≠n", "Andes", "Caribe", "cultura colombiana"

8. **Cultural**:
   Palabras clave: "arte", "literatura", "m√∫sica", "tradici√≥n", "costumbres"

9. **Pol√≠tico**:
   Palabras clave: "gobierno", "elecciones", "pol√≠tica p√∫blica", "partidos pol√≠ticos", "constituci√≥n"

10. **Econ√≥mico**:
    Palabras clave: "mercado", "inflaci√≥n", "PIB", "finanzas", "econom√≠a"

Incluir estas palabras clave en tu consulta puede ayudar a MALLO a aplicar el prompt especializado m√°s adecuado, lo que potencialmente mejorar√° la precisi√≥n y relevancia de las respuestas.

### Mejores Pr√°cticas

1. **S√© Espec√≠fico**: Cuanto m√°s espec√≠fica sea tu consulta, mejor podr√° MALLO seleccionar los agentes y prompts adecuados.

2. **Contexto**: Proporciona contexto adicional cuando sea necesario. Por ejemplo, si est√°s haciendo una pregunta sobre historia colombiana, menciona expl√≠citamente "en el contexto de la historia de Colombia".

3. **Usa Terminolog√≠a Relevante**: Incluir t√©rminos t√©cnicos o espec√≠ficos del campo puede ayudar a MALLO a identificar mejor el dominio de conocimiento.

4. **Solicita An√°lisis**: Si necesitas un an√°lisis m√°s profundo, puedes incluir frases como "analiza en profundidad" o "proporciona un an√°lisis detallado".

5. **Pide M√∫ltiples Perspectivas**: Si deseas que MALLO utilice m√∫ltiples agentes, puedes solicitar expl√≠citamente "diferentes perspectivas" o "an√°lisis desde varios √°ngulos".

Recuerda que MALLO est√° dise√±ado para ser intuitivo y adaptativo, pero proporcionar estas pistas adicionales puede ayudar a obtener respuestas m√°s precisas y relevantes.

## Arquitectura del Sistema

MALLO utiliza una arquitectura de microservicios basada en agentes, donde cada agente (modelo de lenguaje o servicio especializado) puede procesar consultas de manera independiente. El componente central, `AgentManager`, orquesta estos agentes bas√°ndose en la complejidad de la consulta, el tipo de prompt y la disponibilidad de recursos.

### Componentes Clave:
1. **Orquestador de Agentes**: Selecciona y gestiona los agentes apropiados para cada consulta, con sistema de respaldo.
2. **Evaluador de Complejidad y Tipo**: Analiza las consultas para determinar su complejidad, tipo de prompt y requisitos espec√≠ficos.
3. **Sistema de Cach√©**: Almacena y recupera respuestas para consultas frecuentes, con inicializaci√≥n eficiente de recursos.
4. **M√≥dulo de B√∫squeda Web**: Enriquece las respuestas con informaci√≥n actual de la web usando m√∫ltiples proveedores.
5. **Interfaz de Usuario**: Proporciona una experiencia interactiva con personalizaci√≥n de etapas y selecci√≥n de modelos.
6. **Cargador de Modelos**: Gestiona la carga din√°mica de modelos desde diferentes fuentes (OpenRouter, Groq, Ollama).

## Componentes Principales

### agents.py
- **Clase AgentManager**:
  - Gestiona la inicializaci√≥n y selecci√≥n de agentes.
  - Implementa la l√≥gica de procesamiento de consultas.
  - Maneja la integraci√≥n con diferentes APIs de LLM.
  - Sistema de respaldo para modelos que fallan.

### utilities.py
- Funciones para evaluaci√≥n de complejidad de consultas.
- Implementaci√≥n de b√∫squeda web con m√∫ltiples proveedores.
- Sistema de cach√© para respuestas.
- Funciones de logging y manejo de errores.
- Evaluaci√≥n de tipo de prompt y requisitos.

### main.py
- Implementa la interfaz de usuario con Streamlit.
- Gestiona el flujo principal de la aplicaci√≥n.
- Procesa las entradas del usuario y muestra las respuestas.
- Sistema de pesta√±as para organizar la informaci√≥n.
- Personalizaci√≥n de etapas de procesamiento.

### model_loader.py
- Carga din√°mica de modelos desde diferentes fuentes.
- Integraci√≥n con OpenRouter para modelos gratuitos.
- Acceso a todos los modelos disponibles en Groq.
- Carga de modelos locales desde Ollama.

## Flujo de Trabajo

1. El usuario ingresa una consulta a trav√©s de la interfaz de Streamlit.
2. La consulta se eval√∫a para determinar su complejidad, tipo de prompt y requisitos.
3. Se selecciona el agente o conjunto de agentes m√°s apropiados basado en el tipo de consulta.
4. Si es necesario, se realiza una b√∫squeda web para enriquecer el contexto.
5. Los agentes seleccionados procesan la consulta, con sistema de respaldo si alguno falla.
6. Se eval√∫a √©ticamente la respuesta y se refina si es necesario.
7. Si est√° activado, se realiza un meta-an√°lisis para sintetizar m√∫ltiples perspectivas.
8. La respuesta final se presenta al usuario junto con detalles del proceso en pesta√±as organizadas.

## APIs y Servicios Integrados

MALLO integra varios servicios de LLM y APIs, incluyendo:
- OpenAI (GPT-4, GPT-3.5)
- Groq (Llama 3, Mixtral, Gemma)
- Together (Llama 3, Falcon, Yi)
- DeepInfra (Llama 3, Mistral)
- Anthropic (Claude 3)
- DeepSeek (DeepSeek Coder)
- Mistral (Mistral Large, Medium, Small)
- Cohere (Command R+, Command R)
- Ollama (modelos locales)
- OpenRouter (acceso a modelos gratuitos y de pago)

Cada servicio se inicializa y gestiona a trav√©s de la clase `AgentManager` y el m√≥dulo `model_loader.py`, permitiendo una f√°cil expansi√≥n a nuevos proveedores en el futuro. Los modelos se organizan por proveedor con nombres descriptivos para facilitar su selecci√≥n.

## Manejo de Errores y Logging

El sistema implementa un robusto sistema de logging y manejo de errores:
- Los errores se registran en el archivo `mallo.log`.
- Se utilizan diferentes niveles de logging (INFO, WARNING, ERROR) para categorizar los eventos.
- Los errores cr√≠ticos se muestran al usuario a trav√©s de la interfaz de Streamlit.
- Sistema de verificaci√≥n previa de disponibilidad de APIs para prevenir errores.
- Manejo robusto de errores para APIs externas con mensajes detallados y √∫tiles.
- Recuperaci√≥n autom√°tica mediante sistemas de respaldo cuando un modelo o servicio falla.

## Optimizaci√≥n y Cach√©

Para mejorar el rendimiento y reducir la carga en las APIs externas, MALLO implementa:
- Un sistema de cach√© para almacenar respuestas frecuentes.
- Inicializaci√≥n eficiente de componentes con @st.cache_resource.
- Evaluaci√≥n de complejidad y tipo de prompt para optimizar el uso de recursos.
- Selecci√≥n inteligente de agentes basada en el tipo de consulta, disponibilidad y rendimiento hist√≥rico.
- Sistema de respaldo para modelos que fallan, garantizando respuestas incluso cuando algunos servicios no est√°n disponibles.
- Carga diferida de recursos no cr√≠ticos para mejorar el tiempo de inicio.

## Pruebas

El proyecto incluye un conjunto de pruebas unitarias y de integraci√≥n en el directorio `tests/`. Para ejecutar las pruebas:

```
python -m unittest discover tests
```

## Contribuci√≥n

Las contribuciones son bienvenidas. Por favor, sigue estos pasos:

1. Haz fork del repositorio.
2. Crea una nueva rama (`git checkout -b feature/AmazingFeature`).
3. Realiza tus cambios y haz commit (`git commit -m 'Add some AmazingFeature'`).
4. Push a la rama (`git push origin feature/AmazingFeature`).
5. Abre un Pull Request.

## Registro de Cambios

Consulta [CHANGELOG.md](CHANGELOG.md) para ver el historial detallado de cambios del proyecto.

### Versiones Recientes

- **v2.8.0** (15/04/2025): Implementaci√≥n de personalizaci√≥n de etapas de procesamiento, selecci√≥n de modelos y mejoras en la interfaz de usuario.
- **v2.7.0** (14/04/2025): Implementaci√≥n de sistema robusto de manejo de errores para OpenRouter API y mejora de la interfaz de usuario.
- **v2.6.0** (11/04/2025): Integraci√≥n de nuevos modelos avanzados de OpenRouter y soporte para modelos multimodales.
- **v2.5.0** (22/02/2025): Implementaci√≥n de sistema de cach√© optimizado para componentes core y mejoras de rendimiento.

## ExperimentaLABs

MALLO incluye ahora caracter√≠sticas experimentales dise√±adas para mejorar la calidad de las instrucciones y respuestas mediante un proceso de reflexi√≥n iterativa. Estas caracter√≠sticas est√°n disponibles a trav√©s de la funci√≥n `process_user_input_experimental`.

### MALLOEnhancer

MALLOEnhancer es una nueva clase que implementa un enfoque de reflexi√≥n y mejora utilizando m√∫ltiples modelos de lenguaje. Esta caracter√≠stica est√° dise√±ada para:

- Mejorar iterativamente las instrucciones y respuestas.
- Utilizar m√∫ltiples APIs de LLM para obtener diversas perspectivas.
- Evaluar la calidad de las instrucciones y respuestas mediante m√©tricas como IFD y r-IFD.

Para habilitar estas caracter√≠sticas experimentales, use la bandera `--experimental` al iniciar la aplicaci√≥n:

```
streamlit run main.py -- --experimental
```

## Roadmap

Nuestros planes futuros para MALLO incluyen:

1. Refinar y optimizar el proceso de reflexi√≥n iterativa en MALLOEnhancer.
2. Mejorar la integraci√≥n y el uso eficiente de m√∫ltiples APIs de LLM.
3. Desarrollar m√©tricas m√°s avanzadas para la evaluaci√≥n de la calidad de instrucciones y respuestas.
4. Implementar un sistema de aprendizaje continuo basado en el feedback de los usuarios.
5. Explorar la posibilidad de fine-tuning de modelos basados en los resultados de MALLOEnhancer.

Estas caracter√≠sticas experimentales representan nuestro compromiso con la mejora continua de MALLO y la exploraci√≥n de nuevas formas de optimizar la interacci√≥n entre humanos y LLMs.

## Licencia

Este proyecto est√° licenciado bajo la Licencia MIT - vea el archivo [LICENSE](LICENSE) para m√°s detalles.

## Contacto

Alexander Oviedo Fadul

[GitHub](https://github.com/bladealex9848) | [Website](https://alexanderoviedofadul.dev/) | [Instagram](https://www.instagram.com/alexander.oviedo.fadul) | [Twitter](https://twitter.com/alexanderofadul) | [Facebook](https://www.facebook.com/alexanderof/) | [WhatsApp](https://api.whatsapp.com/send?phone=573015930519&text=Hola%20!Quiero%20conversar%20contigo!) | [LinkedIn](https://www.linkedin.com/in/alexander-oviedo-fadul/)